"""
AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯é…å»¶ãƒ­ãƒ¼ãƒ‰ï¼ˆLazy Loadingï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€èµ·å‹•æ™‚é–“ã‚’æœ€é©åŒ–ã—ã¦ã„ã¾ã™ã€‚
ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã¯åˆå›ã®é–¢æ•°å‘¼ã³å‡ºã—æ™‚ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€ä»¥é™ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

å®Ÿè£…ã®è©³ç´°:
- sentence_transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯åˆå›å‘¼ã³å‡ºã—æ™‚ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
- SentenceTransformerãƒ¢ãƒ‡ãƒ«ã¯åˆå›å‘¼ã³å‡ºã—æ™‚ã«ãƒ­ãƒ¼ãƒ‰
- åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã¯åˆå›å‘¼ã³å‡ºã—æ™‚ã«ãƒ­ãƒ¼ãƒ‰
- 2å›ç›®ä»¥é™ã®å‘¼ã³å‡ºã—ã§ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨

ã“ã®è¨­è¨ˆã«ã‚ˆã‚Šã€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯å³åº§ã«å®Œäº†ã—ã€
Botèµ·å‹•æ™‚é–“ãŒå¤§å¹…ã«çŸ­ç¸®ã•ã‚Œã¾ã™ã€‚
"""

import json
import os
import threading

from gemini_config import create_generative_model

EMBED_PATH = os.path.join(os.path.dirname(__file__), "../data/embeddings.json")
PROMPTS_PATH = os.path.join(os.path.dirname(__file__), "../config/prompts.yaml")

# é…å»¶ãƒ­ãƒ¼ãƒ‰ç”¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
_model = None
_texts = None
_embeddings = None
_prompts = None
_cached_additional_role = None  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸè¿½åŠ å½¹å‰²ã®å€¤
_gemini_model = None  # Gemini APIãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
_gemini_module = None  # genaiãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
_safety_settings = None  # å®‰å…¨æ€§è¨­å®šã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
_llm_first_success = False  # LLMåˆå›æˆåŠŸãƒ•ãƒ©ã‚°
_initialized = False
_init_lock = threading.Lock()
_llm_success_lock = threading.Lock()  # LLMæˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤ºç”¨ãƒ­ãƒƒã‚¯


def is_initialized():
    """
    åˆæœŸåŒ–æ¸ˆã¿ã‹ã©ã†ã‹ã‚’è¿”ã™

    Returns:
        bool: åˆæœŸåŒ–æ¸ˆã¿ã®å ´åˆTrueã€æœªåˆæœŸåŒ–ã®å ´åˆFalse
    """
    return _initialized


def ensure_initialized_with_callback(callback=None):
    """
    åˆæœŸåŒ–ã‚’å®Ÿè¡Œã—ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’é€šã˜ã¦åˆå›åˆæœŸåŒ–ã‹ã©ã†ã‹ã‚’é€šçŸ¥ã™ã‚‹

    ã“ã®é–¢æ•°ã¯åˆæœŸåŒ–å‡¦ç†ã‚’å®Ÿè¡Œã—ã€åˆå›ã®åˆæœŸåŒ–æ™‚ã®ã¿ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚
    2å›ç›®ä»¥é™ã®å‘¼ã³å‡ºã—ã§ã¯ä½•ã‚‚ã›ãšã€å³åº§ã«Trueã‚’è¿”ã—ã¾ã™ã€‚

    Args:
        callback: åˆå›åˆæœŸåŒ–æ™‚ã«å‘¼ã³å‡ºã•ã‚Œã‚‹é–¢æ•°ï¼ˆå¼•æ•°ãªã—ï¼‰

    Returns:
        bool: æ—¢ã«åˆæœŸåŒ–æ¸ˆã¿ã ã£ãŸå ´åˆTrueã€ä»Šå›åˆã‚ã¦åˆæœŸåŒ–ã—ãŸå ´åˆFalse

    Raises:
        FileNotFoundError: EMBED_PATHãŒå­˜åœ¨ã—ãªã„å ´åˆ
        json.JSONDecodeError: JSONãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æã«å¤±æ•—ã—ãŸå ´åˆ
        Exception: ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸå ´åˆ
    """
    global _model, _texts, _embeddings, _initialized

    # æ—¢ã«åˆæœŸåŒ–æ¸ˆã¿
    if _initialized:
        return True

    # ãƒ€ãƒ–ãƒ«ãƒã‚§ãƒƒã‚¯ãƒ­ãƒƒã‚­ãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³
    with _init_lock:
        # ãƒ­ãƒƒã‚¯å–å¾—å¾Œã«å†åº¦ãƒã‚§ãƒƒã‚¯
        if _initialized:
            return True

        # åˆå›åˆæœŸåŒ–é–‹å§‹
        if callback:
            callback()

        try:
            # sentence_transformersã‚’é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆèµ·å‹•æ™‚é–“ã®æœ€é©åŒ–ï¼‰
            from sentence_transformers import SentenceTransformer

            # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
            _model = SentenceTransformer("all-MiniLM-L6-v2")

            # åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰
            if not os.path.exists(EMBED_PATH):
                raise FileNotFoundError(
                    f"åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {EMBED_PATH}\n"
                    "prepare_dataset.pyã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
                )

            with open(EMBED_PATH, "r") as f:
                dataset = json.load(f)

            _texts = [item["text"] for item in dataset]
            _embeddings = [item["embedding"] for item in dataset]

            _initialized = True
            return False  # åˆå›åˆæœŸåŒ–å®Œäº†
        except json.JSONDecodeError as e:
            raise Exception(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}") from e
        except Exception as e:
            raise Exception(f"AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}") from e


def _ensure_initialized():
    """
    ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚’é…å»¶ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ï¼ˆåˆå›å‘¼ã³å‡ºã—æ™‚ã®ã¿å®Ÿè¡Œï¼‰

    ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªå®Ÿè£…ã«ã‚ˆã‚Šã€è¤‡æ•°ã®åŒæ™‚å‘¼ã³å‡ºã—ã§ã‚‚å®‰å…¨ã«åˆæœŸåŒ–ã•ã‚Œã¾ã™ã€‚
    ãƒ€ãƒ–ãƒ«ãƒã‚§ãƒƒã‚¯ãƒ­ãƒƒã‚­ãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€é©åŒ–ã—ã¦ã„ã¾ã™ã€‚

    Raises:
        FileNotFoundError: EMBED_PATHãŒå­˜åœ¨ã—ãªã„å ´åˆ
        json.JSONDecodeError: JSONãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æã«å¤±æ•—ã—ãŸå ´åˆ
        Exception: ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸå ´åˆ
    """
    global _model, _texts, _embeddings, _initialized

    # åˆæœŸãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ãƒƒã‚¯ãªã—ï¼‰- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
    if _initialized:
        return

    # ãƒ€ãƒ–ãƒ«ãƒã‚§ãƒƒã‚¯ãƒ­ãƒƒã‚­ãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³
    with _init_lock:
        # ãƒ­ãƒƒã‚¯å–å¾—å¾Œã«å†åº¦ãƒã‚§ãƒƒã‚¯
        if _initialized:
            return

        try:
            # sentence_transformersã‚’é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆèµ·å‹•æ™‚é–“ã®æœ€é©åŒ–ï¼‰
            from sentence_transformers import SentenceTransformer

            # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
            _model = SentenceTransformer("all-MiniLM-L6-v2")

            # åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰
            if not os.path.exists(EMBED_PATH):
                raise FileNotFoundError(
                    f"åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {EMBED_PATH}\n"
                    "prepare_dataset.pyã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
                )

            with open(EMBED_PATH, "r") as f:
                dataset = json.load(f)

            _texts = [item["text"] for item in dataset]
            _embeddings = [item["embedding"] for item in dataset]

            _initialized = True
        except FileNotFoundError:
            raise
        except json.JSONDecodeError as e:
            raise Exception(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}") from e
        except Exception as e:
            raise Exception(f"AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}") from e


def _load_prompts():
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šï¼‰

    ç’°å¢ƒå¤‰æ•° ADDITIONAL_AGENT_ROLE ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€
    ãã®å†…å®¹ã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ ã—ã¾ã™ã€‚
    ãŸã ã—ã€ç’°å¢ƒå¤‰æ•°ãŒç©ºæ–‡å­—åˆ—ã¾ãŸã¯ç©ºç™½ã®ã¿ã®å ´åˆã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚

    ç’°å¢ƒå¤‰æ•°ãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯è‡ªå‹•çš„ã«ç„¡åŠ¹åŒ–ã•ã‚Œã€
    æ–°ã—ã„å€¤ãŒåæ˜ ã•ã‚Œã¾ã™ã€‚

    Returns:
        dict: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š

    Raises:
        FileNotFoundError: prompts.yamlãŒå­˜åœ¨ã—ãªã„å ´åˆ
        RuntimeError: YAMLæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆ
    """
    global _prompts, _cached_additional_role

    # ç’°å¢ƒå¤‰æ•°ã®ç¾åœ¨ã®å€¤ã‚’å–å¾—
    current_additional_role = os.environ.get("ADDITIONAL_AGENT_ROLE", "").strip()

    # ç’°å¢ƒå¤‰æ•°ãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–
    if _prompts is not None and _cached_additional_role != current_additional_role:
        _prompts = None
        print("ğŸ”„ è¿½åŠ ã®å½¹å‰²è¨­å®šãŒå¤‰æ›´ã•ã‚Œã¾ã—ãŸã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã™")

    if _prompts is None:
        prompts_path = os.path.abspath(PROMPTS_PATH)
        if not os.path.exists(prompts_path):
            raise FileNotFoundError(
                f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {prompts_path}\n"
                "config/prompts.yamlã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚"
            )

        import yaml

        try:
            with open(prompts_path, "r", encoding="utf-8") as f:
                _prompts = yaml.safe_load(f)
        except yaml.YAMLError as e:
            raise RuntimeError(
                f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ{prompts_path}ï¼‰ã®YAMLæ§‹æ–‡ã«èª¤ã‚ŠãŒã‚ã‚Šã¾ã™ã€‚\n"
                f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {e}"
            ) from e

        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¿½åŠ ã®å½¹å‰²æŒ‡å®šã‚’èª­ã¿è¾¼ã‚€
        if current_additional_role and _prompts:
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ ã®å½¹å‰²ã‚’çµ±åˆ
            if "llm_system_prompt" in _prompts:
                _prompts["llm_system_prompt"] = (
                    f"{_prompts['llm_system_prompt']}\n\n"
                    f"ã€è¿½åŠ ã®å½¹å‰²ãƒ»æ€§æ ¼ã€‘\n{current_additional_role}"
                )
                print("âœ… è¿½åŠ ã®å½¹å‰²è¨­å®šãŒé©ç”¨ã•ã‚Œã¾ã—ãŸ")

        # ç¾åœ¨ã®ç’°å¢ƒå¤‰æ•°ã®å€¤ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        _cached_additional_role = current_additional_role
    return _prompts


def generate_response_with_llm(query, similar_messages):
    """
    LLM APIã‚’ä½¿ç”¨ã—ã¦ã€éå»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ–‡è„ˆã¨ã—ã¦å¿œç­”ã‚’ç”Ÿæˆ

    Args:
        query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        similar_messages: é¡ä¼¼åº¦ã®é«˜ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ

    Returns:
        tuple: (response: str or None, error_message: str or None)
            - response: LLMãŒç”Ÿæˆã—ãŸå¿œç­”æ–‡å­—åˆ—ã€ã¾ãŸã¯Noneï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ï¼‰
            - error_message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€ã¾ãŸã¯Noneï¼ˆæˆåŠŸæ™‚ï¼‰
    """
    global _gemini_model, _gemini_module, _safety_settings

    # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from llm_error_handler import (
        MAX_RETRIES,
        log_llm_request,
        log_llm_response,
        should_retry_with_backoff,
        wait_for_retry,
    )

    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key or not api_key.strip():
        # APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯Noneã‚’è¿”ã™
        # (èµ·å‹•æ™‚ã«æ—¢ã«æ¡ˆå†…æ¸ˆã¿)
        return None, None

    # ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦å†åˆ©ç”¨ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šï¼‰
    if _gemini_model is None:
        # Gemini APIãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        _gemini_module, _gemini_model, _safety_settings = create_generative_model(
            api_key
        )

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—
    genai = _gemini_module
    safety_settings = _safety_settings

    # æ–‡è„ˆã¨ã—ã¦éå»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ•´å½¢
    context = "\n".join([f"- {msg}" for msg in similar_messages[:5]])

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã‚’èª­ã¿è¾¼ã¿
    prompts = _load_prompts()

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰ï¼ˆå¤–éƒ¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
    prompt = f"""{prompts['llm_system_prompt']}

{prompts['llm_context_header']}
{context}

{prompts['llm_query_header']}
{query}

{prompts['llm_response_header']}
{prompts['llm_response_instruction']}"""

    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒ­ã‚°ã«è¨˜éŒ²
    log_llm_request(query, len(similar_messages[:5]))

    # ãƒªãƒˆãƒ©ã‚¤ãƒ«ãƒ¼ãƒ—
    last_error_message = None
    for attempt in range(MAX_RETRIES + 1):
        try:
            # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’æ˜ç¤ºçš„ã«è¨­å®šï¼‰
            response = _gemini_model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=1000,
                ),
                safety_settings=safety_settings,
                request_options={"timeout": 30},
            )

            if response and response.text:
                result = response.text.strip()
                log_llm_response(True, len(result))
                # åˆå›ã®LLMå¿œç­”æˆåŠŸæ™‚ã«ã®ã¿ç¢ºèªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰
                with _llm_success_lock:
                    global _llm_first_success
                    if not _llm_first_success:
                        print(
                            "âœ… LLM APIå¿œç­”æˆåŠŸ: Gemini APIã‚’ä½¿ç”¨ã—ã¦å¿œç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™"
                        )
                        _llm_first_success = True
                return result, None

            error_msg = "LLM APIã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã—ãŸ"
            print(f"âš ï¸ {error_msg}")
            log_llm_response(False)
            return None, error_msg

        except Exception as e:
            # ä¾‹å¤–ã‚’è©•ä¾¡ã—ã€ãƒªãƒˆãƒ©ã‚¤ã™ã¹ãã‹åˆ¤æ–­
            retry_info = should_retry_with_backoff(e, attempt)
            should_retry, wait_time, user_message = retry_info
            last_error_message = user_message

            if should_retry:
                wait_for_retry(wait_time)
                continue
            else:
                # ãƒªãƒˆãƒ©ã‚¤ä¸å¯ã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
                error_msg = f"LLM APIå‘¼ã³å‡ºã—ã«å¤±æ•—: {type(e).__name__}: {str(e)}"
                print(f"âš ï¸ {error_msg}")
                log_llm_response(False)
                return None, user_message

    # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ãŸå ´åˆ
    error_msg = f"LLM API: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°({MAX_RETRIES}å›)ã«é”ã—ã¾ã—ãŸ"
    print(f"âš ï¸ {error_msg}")
    log_llm_response(False)
    return None, last_error_message if last_error_message else error_msg


# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æœ€ã‚‚è¿‘ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ¤œç´¢


def search_similar_message(query, top_k=3):
    _ensure_initialized()
    # utilã‚’é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from sentence_transformers import util

    query_emb = _model.encode(query)
    scores = util.cos_sim(query_emb, _embeddings)[0]
    top_results = scores.argsort(descending=True)[:top_k]
    return [_texts[i] for i in top_results]


def generate_response(query, top_k=5):
    """
    ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ã€LLM APIã‚’ä½¿ç”¨ã—ã¦éå»ã®çŸ¥è­˜ã‚’åŸºã«è¿”ä¿¡ã‚’ç”Ÿæˆ

    Args:
        query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        top_k: å‚è€ƒã«ã™ã‚‹é¡ä¼¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ•°

    Returns:
        ç”Ÿæˆã•ã‚ŒãŸè¿”ä¿¡æ–‡å­—åˆ—

    Raises:
        ValueError: GEMINI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆ
    """
    _ensure_initialized()

    # APIã‚­ãƒ¼ã®ç¢ºèª
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key or not api_key.strip():
        raise ValueError(
            "GEMINI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n"
            "GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        )

    # é¡ä¼¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ¤œç´¢
    similar_messages = search_similar_message(query, top_k)

    # é¡ä¼¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
    if not similar_messages:
        raise ValueError(
            "é–¢é€£ã™ã‚‹éå»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n"
            "çŸ¥è­˜ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ãç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )

    # LLM APIã‚’ä½¿ç”¨ã—ã¦å¿œç­”ã‚’ç”Ÿæˆ
    llm_response, error_message = generate_response_with_llm(query, similar_messages)
    if llm_response:
        return llm_response

    # LLM APIãŒå¤±æ•—ã—ãŸå ´åˆ
    if error_message:
        raise RuntimeError(f"LLM APIã‹ã‚‰ã®å¿œç­”å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n{error_message}")
    else:
        raise RuntimeError(
            "LLM APIã‹ã‚‰ã®å¿œç­”å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n"
            "APIã‚­ãƒ¼ãŒæ­£ã—ã„ã‹ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )


# ãƒ†ã‚¹ãƒˆç”¨
if __name__ == "__main__":
    q = input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")

    # é¡ä¼¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º
    print("\n--- é¡ä¼¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ ---")
    results = search_similar_message(q)
    for r in results:
        print("-", r)

    # äºˆæ¸¬è¿”ä¿¡ã®ç”Ÿæˆ
    print("\n--- äºˆæ¸¬ã•ã‚Œã‚‹è¿”ä¿¡ ---")
    response = generate_response(q)
    print(response)
